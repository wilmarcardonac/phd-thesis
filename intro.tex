\addcontentsline{toc}{chapter}{Introduction}
\chapter*{Introduction}
\label{intro} 

The $20$th century gradually saw the emergence of the standard model of cosmology. A linear relation between distances and recession velocities of galaxies \cite{Hubble:1929ig}, the observed abundance of chemical elements in the universe \cite{Gamow:1946eb,Alpher:1948ve,Gamow:1949zz,Alpher:1950zz}, and the existence of the Cosmic Microwave Background radiation (CMB) \cite{Alpher:1950zz,Penzias:1965wn} evidenced a dynamical rather than static universe: the universe is expanding. Observations of type Ia supernovae in $1998$ \cite{Riess:1998cb,Perlmutter:1998np} modified a ``little bit'' the picture, setting one of the most important problems cosmologists will be addressing in the $21$st century: the universe is not only expanding, it is speeding up and cosmologists want to find out why. 

The cosmological principle -- the assumption that the universe at sufficiently large scales is homogeneous and isotropic -- is one of the cornerstones of the concordance model of cosmology \cite{Robertson:1935zz,Walker1937}. If one assumes there is no charge asymmetry in the universe \cite{Caprini:2003gz}, the only relevant interaction on large scales is gravity. In the vanilla model of cosmology the gravitational interaction is described by Einstein's General Relativity. Solutions for Einstein's field equations -- that couple geometry to both matter-energy and pressure -- satisfying the cosmological principle are known the since early $20$th century \cite{Friedman:1922kd,Friedmann:1924bb,Lemaitre:1927zz,Lemaitre:1931zz}. In those solutions -- the so-called FLRW metric -- the expansion of the universe is given by the scale factor $a(t)$, a function which depends on the cosmic time $t$ and scales the distance between two given points as the universe expands. The matter content in the standard model of cosmology is only partially given by particles in the Standard Model (SM) of particle physics (i.e., photons, electrons, and so on). The remaining matter, dubbed Cold Dark Matter (CDM) because it only seems to interact with the baryonic matter through gravity, is required, for instance, to fit observations of galaxy velocities in galaxy clusters \cite{Zwicky:1933gu}. Finally, in order to describe the accelerated expansion of the universe, the concordance model of cosmology reintroduces the cosmological constant $\Lambda$ (first introduced by Albert Einstein in the early $20$th century). The standard model of cosmology is thus named  $\Lambda CDM$ model.

The universe is obviously not completely homogeneous since inhomogeneities exist, such as galaxies and clusters of galaxies. Moreover, both the convergence of observations into a flat universe and the fact that the CMB appears to be incredibly uniform in regions now causally disconnected  make the $\Lambda CDM$ model an incomplete description of the universe. These three main difficulties of the standard model of cosmology (i.e., structure formation, horizon, flatness) can be solved by adding an inflationary epoch to the history of the universe \cite{Guth:2005zr}. In the simplest inflationary scenarios the potential energy of a scalar field drives an exponential expansion in the very early universe rendering the universe extremely flat very quickly (within about $10^{-35}\, \second$). The universe would have evolved from a tiny patch (about $10^{-26}\,\mathrm{m}$) where regions that are today are causally isolated were then in causal contact thus solving the horizon problem of the standard model of cosmology. Perhaps most importantly, inflationary models predict that quantum fluctuations of the scalar field in the early stage of the universe would have seeded the density fluctuations we observe today in the form of galaxies, galaxy clusters, and CMB fluctuations.

Over the past three decades cosmology has witnessed the come of an age of precision. Full sky CMB experiments such as Cosmic Background Explorer (COBE) \cite{Smoot:1992td}, Wilkinson Microwave Anisotropy Probe (WMAP) \cite{Bennett:2003bz}, and PLANCK satellite \cite{Ade:2013sjv} have measured CMB anisotropies in different frequencies and on a wide range of angular scales thus allowing a careful study of the predictions made by the inflationary $\Lambda CDM$ model. The CMB spectrum matches incredibly well that of a black body with temperature $2.7\,\mathrm{K}$ as predicted by the standard model \cite{Alpher:1950zz}. By investigating CMB fluctuations cosmologists have been able to constraint to high accuracy the curvature of the universe: it agrees pretty well with the flatness prediction of inflationary models \cite{Ade:2015xua}. Although rather controversial, there is no compelling evidence for significant deviations of the cosmological principle in the Planck CMB data \cite{Ade:2015hxq}. Furthermore, current CMB experiments support the existence of dark matter and also the accelerated expansion of the universe. Galaxy surveys such as Sloan Digital Sky Survey (SDSS) have also played an important role in testing cosmological models \cite{Tegmark:2003uf,Tegmark:2003ud}. Partially mapping the distribution of mass in the universe, using observations of $\approx 10^6$ galaxies with mean red shift $z \approx 0.1$, SDSS collaboration detected a baryon acoustic peak which is an imprint of the recombination-epoch acoustic oscillations on the low-redshift clustering of matter \cite{Eisenstein:2005su}. This detection confirms a prediction of the standard cosmological theory. Upcoming galaxy surveys offer thus an important complementary probe and will be key for testing cosmological models in the near future, for instance to determine the neutrino mass.

The current concordance cosmological model, although both simple and a good fit for current data sets, lacks in fundamental grounds. On the one hand, it assumes the existence of dark matter which thus far has not been directly observed; the only known dark matter candidate being neutrinos. On the other hand, the cosmological problem poses a serious conundrum for quantum field theory which is unable to explain the extremely tiny observed value of the vacuum energy. Numerous alternative approaches to explain the accelerated expansion of the universe from first principles have been proposed over the past years. In one family of models evolving scalar fields -- easily found in fundamental theories of matter -- have been used to model dark energy as a fluid driving the late-time acceleration \cite{Copeland:2006wr}. Another family of models exploits the ambiguity of the cosmological constant in the Einstein field equations and proposes that modifications of General Relativity -- the theory of gravity in the concordance model -- could be the reason that accounts for the speeding up of the universe \cite{Clifton:2011jh}. Therefore although remarkable progress has been made on both theory and observation, degeneracies at the model level\footnote{The situation is even worst taking into account the number of inflationary scenarios which are compatible with current observations. Although non-Gaussianity of CMB fluctuations was expected to break the degeneracy in inflationary models, the 2015 Planck results \cite{Ade:2015ava,Ade:2015lrj} showed that there are still several inflationary models compatible with observations.} are still present and efficient ways to discriminate cosmological models are needed.   

In Chapter \ref{chapter-ade} of this thesis Lukas Hollenstein, Martin Kunz and I have considered one possibility for breaking degeneracies at the model level. Dark energy anisotropic stress is a key feature as it allows to discriminate the standard dynamical dark energy model -- a scalar field minimally coupled to gravity-- from the so-called modified gravity models. In linear theory, the former class of models does not support any anisotropic stress whereas models such as scalar-tensor and $f(R)$ generically have a non-zero anisotropic stress. We have adopted a phenomenological approach and studied a model of anisotropic dark energy which encompasses both internally and externally sourced anisotropic stress, that additionally allows for a scale dependence \cite{Cardona:2014iba}. In particular, we have investigated how the presence of a non-zero dark energy anisotropic stress impacts both the dark matter and dark energy perturbations, as well as CMB angular power spectrum. We found approximate solutions for both dark matter and dark energy perturbations in some particular scenarios and constrained dark energy anisotropic stress parameters with recent data sets. 

Gleaning information about cosmological parameters from all the available data sets will surely shed light on the shortcomings of the $\Lambda CDM$ model. Indeed, upcoming galaxy surveys such as Dark Energy Survey \href{www.darkenergysurvey.org}{(DES)}, Dark Energy Spectroscopic Instrument \href{http://desi.lbl.gov/}{(DESI)}, Large Synoptic Survey Telescope \href{www.lsst.org}{(LSST)}, Physics of the Accelerating Universe Survey \href{www.pausurvey.org}{(PAUS)}, and \href{www.euclid-ec.org}{EUCLID} will play a key role in understanding the accelerated expansion of the universe and constraint the neutrino masses. The come of all this new data will make necessary very careful analyses and appropriate modelling of the statistical properties of the matter density field. In particular, since those galaxy surveys will probe scales comparable to the horizon, analyses must properly include relevant relativistic effects. In Chapter \ref{chapter-mnu} of this thesis, Ruth Durrer, Martin Kunz, Francesco Montanari and I have investigated the impact of neglecting lensing convergence when analysing data from a EUCLID-like survey. We have shown that neglecting lensing convergence when constraining neutrino masses, for instance, would lead to spurious detection of their absolute mass scale, thus hindering one of the key foals of future surveys \cite{Cardona:2016qxn}. Moreover, we found that since biases of cosmological parameters in analyses neglecting lensing might reach several standard deviations, the usual linear approximation in Fisher matrix formalisms breaks down,  therefore it might no longer be appropriate. We have then adopted a Markov Chain Monte Carlo (MCMC) approach to yield reliable forecasts.

Reliable, accurate, model independent measurements of the Hubble constant $H_0$ are essential to understand the physics behind the phenomenologically successful $\Lambda CDM$ model. Accurate and precise determinations of $H_0$ will make possible to put tighter constraints on dark energy parameters -- such as the equation of state for dark energy $w$ -- and the mass of neutrinos. Although direct measurements of $H_0$ have proven to be difficult (e.g., control of systematic errors, relatively small data sets, fully consistency of different methods for measuring distances), remarkable progress has been achieved over past decades; improvements include an enlarged sample of $\SNe$ hosts having a Cepheid calibrated distance, reduction of uncertainties on anchor distances, and increase of infra-red observations of Cepheid stars. All these efforts have yielded a direct $H_0$ measurement almost as precise as the indirect determination for the $\Lambda CDM$ model derived by the Planck collaboration \cite{Ade:2015xua,Riess:2016jrr}. The group led by Adam Riess has recently found a $H_0$ value which is in a $\approx 3\sigma$ disagreement with that derived from CMB measurements \cite{Riess:2016jrr}. The reasons underlying this tension are unclear (e.g., remaining CMB systematics, issues with utilised statistical methods), but if the disagreement is proved robust, it might signify new physics.  In Chapter \ref{chapter-h0} of this thesis, Martin Kunz, Valeria Pettorino, and I have developed a statistical method using Bayesian hyper-parameters to measure the Hubble constant with the available data. The method allows a comprehensive treatment of the available data sets with no need for arbitrary outlier rejection algorithms. Our measurement of $H_0$ is a bit less precise than that by Riess et al. \cite{Riess:2016jrr}, but we understand this as a result of inconsistencies in the data sets. 