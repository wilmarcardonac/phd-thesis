\chapter{Outlook}
\label{chapter-outlook}

Almost two decades after his discovery in $1998$ the observed acceleration of the universe remains one the most important questions in cosmology. The current concordance cosmological model, although both simple and providing a good fit for the universe's expansion, lacks in fundamental grounds. On the one hand, physicists have been making a huge effort to find direct evidence of the dark matter, but thus far no detection has been observed. On the other hand, the incredibly small value of the observed vacuum energy has eluded an explanation within the quantum field theory framework. 

Some alternative approaches to explain the accelerating expansion of the universe from first principles have been proposed over the past years (e.g., evolving scalar fields models, modified gravity models, higher dimensions), but degeneracies at the model level are still present. Although analyses of current data sets have allowed to shrink the allowed regions in parameter space, this effort has not been enough to rule out most of the alternatives to the concordance model of cosmology. This situation is however expected to change with the come of new data from the new generation of both ground-based and space-based experiments; future galaxy surveys like Euclid which will map the distribution of matter in the universe at scales comparable to the horizon; \commentr{polarisation, supernovae projects, MW parallaxes Gaia} . Precise measurements of the Hubble constant $H_0$, the equation of state of the dark energy $w$, and the matter power spectrum would allow to conclusively discriminate $\Lambda$CDM from its alternatives. 

Precise measurements will require careful modelling and statistical methods as good as the data sets, otherwise the analyses could lead to biased results and underestimation of error bars. In Chapter \ref{chapter-h0} Martin Kunz, Valeria Pettorino, and I have applied a Bayesian method to determine the current expansion rate of the universe $H_0$. We have done a detailed analysis of samples of Cepheid variables and $\SNe$ hosts in \cite{Riess:2011yx,Riess:2016jrr} showing that the precision of our measurements depends on the consistency of the data sets. We found our measurements of $H_0$ to be in good agreement with those by Riess et al. \cite{Riess:2011yx,Riess:2016jrr}, but with slightly greater error bars. Our results therefore confirm the disagreement between local and indirect determinations of the Hubble constant. This tension could arise from issues with the statistical method used in the determination of $H_0$, be a hint of remaining CMB systematics \cite{Riess:2016jrr}, or signify new physics. Since in Chapter \ref{chapter-h0} we have determined $H_0$ with a different statistical method to that utilised by Riess et al. in \cite{Riess:2016jrr} -- and our results are in good agreement with theirs --, we conclude the current local measurement of $H_0$ is robust against the statistical method used. When adding external data sets to the CMB data, the Planck collaboration utilised a conservative prior given by the analysis of G. Efstathiou \cite{Efstathiou:2013via} which only uses $\NGC$ as an anchor distance. Although Efstathiou's value is consistent, within error bars, with our determination of $H_0$ using the R16 data set, his measurement has greater error bars. The inclusion of Efstathiou's value in the Planck analysis should therefore be revised as it discards data and might hinder our understanding of both cosmological model and data sets.        

An accurate and precise measurement of the Hubble constant $H_0$ will certainly be key for determining the neutrino absolute mass scale. CMB data and local measurements of the Hubble constant, for instance, will help to break degeneracies (e.g., $H_0$ and $m_\nu$) and improve constraints on cosmological parameters from future galaxy surveys.  Ruth Durrer, Martin Kunz, Francesco Montanari, and I have however shown in Chapter \ref{chapter-mnu} that a careful modelling of number counts will be required. In particular, relativistic effects such as lensing convergence will need to be included in analyses of data from future galaxy surveys. Neglecting lensing convergence when constraining neutrino masses, for instance, would lead to spurious detection of their absolute mass scale, hindering thus one of the key goals of future surveys. Since biases of cosmological parameters in analyses neglecting lensing might reach several standard deviations, the usual linear approximation in Fisher matrix formalisms breaks down and might not be appropriate any longer. We have shown that a Markov Chain Monte Carlo approach is suitable for reliable forecasts of neutrino masses. The analysis in Chapter \ref{chapter-mnu} used the number counts angular power spectrum $C_\ell(z,z')$ -- an observable -- to make forecasts for an Euclid-like satellite, took into account all relevant relativistic effects, but included non-linear effects very conservatively analysing scales up to $\ell=400$. Non-linear effects will be key as much of the effect of massive neutrinos on the angular matter power spectrum is brought about on small scales. Thus, a natural extension of the analysis presented in Chapter \ref{chapter-mnu} should divide the galaxy sample in more red-shift bins (we used $5$ red-shift bins), include scales up to $\ell=2000$, and adapt the likelihood to include a theoretical error which would take into account our ignorance on the deep non-linear regime. This extension requires an optimisation of the computing tools utilised in Chapter \ref{chapter-mnu} (i.e., both Boltzmann and MCMC codes) which is already ongoing.   

One of the main scientific objectives of future galaxy surveys is to understand the nature of dark energy and dark matter. By using weak lensing and galaxy clustering, an Euclid-like survey will be able to constrain dark energy parameters and distinguish General Relativity from a wide range of modified gravity theories. Dark energy anisotropic stress is a key feature since it allows to discriminate the standard dynamical dark energy model -- a scalar field minimally coupled to gravity -- from the so-called modified gravity models. In linear theory, the former class of models does not support any anisotropic stress whereas models such as scalar-tensor and $f(R)$ generically have a non-zero anisotropic stress. Determining how well anisotropic dark energy will be constrained by the combination of data from both Planck-like and Euclid-like satellites is therefore an interesting problem and a phenomenological approach might be suitable. In Chapter \ref{chapter-ade}, Lukas Hollenstein, Martin kunz, and I have worked on a phenomenological model of anisotropic dark energy which encompasses both internally and externally sourced anisotropic stress and also allows for a scale dependence. Our model comprehends, for instance, models of dark energy coupled to dark matter, modified gravity models, and isotropic dark energy. In our analysis of Chapter \ref{chapter-ade} we constrained the parameter space of the model using mainly observations of the CMB by the Planck satellite. Large-scale structure observations  were not included in the analysis but in the meanwhile I have developed the tools to include the angular matter power spectrum in the analysis (see Chapter \ref{chapter-mnu}). As we have shown in our study, the dark energy anisotropic stress might act as a source of both dark matter and dark energy perturbations. It is therefore very interesting to investigate how much observations of galaxy clustering could improve our results in Chapter \ref{chapter-ade}. The detection of a non-zero anisotropic stress would play a key role in our understanding of the fundamental causes of the late-time acceleration of the universe. 
