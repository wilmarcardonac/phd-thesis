\chapter{Summary and outlook}
\label{chapter-outlook}

Almost two decades after the discovery of the universe's acceleration in $1998$ its fundamental cause remains largely unknown and one of the most important questions in cosmology. The current concordance cosmological model, although both simple and a good fit for the universe's expansion, lacks in fundamental grounds. On the one hand, physicists have been making a huge effort to find direct evidence of dark matter, but thus far no detection has been observed. On the other hand, the incredibly small value of the observed vacuum energy has eluded an explanation within the quantum field theory framework. 

Some alternative approaches to explain the universe's accelerating expansion from first principles have been proposed over past years (e.g., evolving scalar fields models, modified gravity models, higher dimensions), but degeneracies at the model level remain. Although analyses of current data sets have made it possible to shrink the allowed regions in parameter space, this effort has not been enough to rule out most of the alternatives to the concordance model of cosmology. This situation is however expected to change with the come of new data from the new generation of both ground-based and space-based experiments; future galaxy surveys like Euclid will map the distribution of matter in the universe at scales comparable to the horizon; the James Webb Space Telescope \href{www.jwst.nasa.gov}{(Webb)}, which is the successor of the Hubble Space Telescope, will probably help to augment the sample of $\SNe$ hosts; the \href{http://sci.esa.int/gaia/}{Gaia} mission will surely provide parallax measurements of new Cepheid stars in our galaxy. Precise measurements of the Hubble constant $H_0$, the equation of state of the dark energy $w$, and the matter power spectrum would allow to conclusively discriminate $\Lambda$CDM from its alternatives. 

Precise measurements will require careful modelling and statistical methods as good as the data sets, otherwise the analysis could lead to biased results and underestimation of error bars. In Chapter \ref{chapter-h0}, Martin Kunz, Valeria Pettorino, and I have applied a Bayesian method to determine the universe's current expansion rate $H_0$. Our  detailed analysis of samples of Cepheid variables and $\SNe$ hosts from \cite{Riess:2011yx,Riess:2016jrr} show that the precision of our measurements depends on the consistency of the data sets. We found our measurements of $H_0$ to be in good agreement with those by Riess et al. \cite{Riess:2011yx,Riess:2016jrr}, but with slightly larger error bars. Our results therefore confirm the disagreement between local and indirect determinations of the Hubble constant. This tension could arise from issues with the statistical method used in the determination of $H_0$, be a hint of remaining CMB systematics \cite{Riess:2016jrr}, or signify new physics. Despite using a different statistical method as in Riess et al \cite{Riess:2016jrr}, both results agree very well. This leads to the conclusion that the current local measurement of $H_0$ is robust against method biases.
%Since in Chapter \ref{chapter-h0} we have determined $H_0$ with a different statistical method to that utilised by Riess et al. in \cite{Riess:2016jrr} -- and our results are in good agreement with theirs --, we conclude the current local measurement of $H_0$ is robust against the statistical method used. 
When adding external data sets to the CMB data, the Planck collaboration utilised a conservative prior given by the analysis of G. Efstathiou \cite{Efstathiou:2013via} that only uses $\NGC$ megamaser system as an anchor distance. Although Efstathiou's value is consistent, within error bars, with our determination of $H_0$ using the R16 data set, his measurement has larger error bars. The inclusion of Efstathiou's value in the Planck analysis should therefore be revised as it discards data and might hinder our understanding of both cosmological model and data sets.        

An accurate and precise measurement of the Hubble constant $H_0$ will certainly be key in determining the neutrino absolute mass scale. CMB data and local measurements of the Hubble constant, for instance, will help to break degeneracies (e.g., $H_0$ and $m_\nu$) and improve constraints on cosmological parameters from future galaxy surveys.  Ruth Durrer, Martin Kunz, Francesco Montanari, and I have however shown in Ref. \cite{Cardona:2016qxn} that a careful modelling of number counts will be required. In particular, relativistic effects such as lensing convergence will need to be included in analyses of data from future galaxy surveys. Neglecting lensing convergence when constraining neutrino masses, for instance, would lead to spurious detection of their absolute mass scale, thus hindering one of the key goals of future surveys. Since biases of cosmological parameters in analyses neglecting lensing might reach several standard deviations, the usual linear approximation in Fisher matrix formalisms breaks down and might not be appropriate any longer. We have shown that a Markov Chain Monte Carlo approach is suitable for reliable forecasts of neutrino masses. The analysis in Ref. \cite{Cardona:2016qxn} used the number counts angular power spectrum $C_\ell(z,z')$ -- an observable -- to make forecasts for an Euclid-like satellite. It took into account all relevant relativistic effects, but included non-linear effects very conservatively and analysed scales up to $\ell=400$. Non-linear effects will be key in determining neutrino masses considering that the effect of massive neutrinos on the angular matter power spectrum is brought about on small scales. Therefore, a natural extension of the analysis presented in Chapter \ref{chapter-mnu} should divide the galaxy sample in more red-shift bins. Scales up to $\ell=2000$ should be included and the likelihood adapted for a possible error covering insensitivities to the deep non-linear regime.
%include scales up to $\ell=2000$, and adapt the likelihood to take into account a theoretical error which would reflect our ignorance on the deep non-linear regime. 
This extension requires an optimisation of the computing tools used in Ref. \cite{Cardona:2016qxn} (i.e., both Boltzmann and MCMC codes) which is currently under development.   

One of the main scientific objectives of future galaxy surveys is to understand the nature of dark energy and dark matter. By using weak lensing and galaxy clustering, an Euclid-like survey will be able to constrain dark energy parameters and distinguish General Relativity from a wide range of modified gravity theories. Dark energy anisotropic stress is a key feature since it allows to discriminate the standard dynamical dark energy model -- a scalar field minimally coupled to gravity -- from the so-called modified gravity models. In linear theory, the former class of models does not support any anisotropic stress whereas models such as scalar-tensor and $f(R)$ generically have a non-zero anisotropic stress. Determining how well anisotropic dark energy will be constrained by the combination of data from both Planck-like and Euclid-like satellites is therefore a challenging problem. A phenomenological approach could be suitable to deliver satisfactory results. In Ref. \cite{Cardona:2014iba}, Lukas Hollenstein, Martin Kunz, and I have worked on a phenomenological model of anisotropic dark energy which combines both internally and externally sourced anisotropic stress, that also allows for a scale dependence. Our model comprehends, for instance, models of dark energy coupled to dark matter, modified gravity models, and isotropic dark energy. In our analysis we constrained the parameter space of the model using mainly observations of the CMB from the Planck satellite. Large-scale structure observations  were not included in the analysis but meanwhile the tools to include the angular matter power spectrum have been developed. We have shown that the dark energy anisotropic stress might act as a source of both dark matter and dark energy perturbations. It would therefore be very important to investigate to what extent observations of galaxy clustering could improve our results in Ref. \cite{Cardona:2014iba}. The detection of a non-zero anisotropic stress would play a key role in our understanding of the fundamental causes of the universe's late-time acceleration. 
